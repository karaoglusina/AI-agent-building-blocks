# Custom Embeddings

OpenAI's text-embedding-3-small is excellent. But there are reasons to run your own embeddings: privacy, cost, latency, and domain customization.

## Why This Matters

The first time I saw my search results improve dramatically just by adapting embeddings to my domain, I understood why this matters. General-purpose embeddings treat "machine learning engineer" and "ML engineer" as similar because they learned that from internet text. But they might not understand that "LLM researcher" and "foundation model scientist" are nearly synonymous in the job market.

Domain-adapted embeddings learn these nuances from your data.

For the job market analyzer, custom embeddings could:
- Understand job title variations better
- Group semantically similar skills
- Improve search across different company naming conventions

Plus, local embeddings mean:
- Data never leaves your infrastructure
- No per-token API costs
- Lower latency (5ms vs 50-200ms)

## The Key Ideas

### Sentence Transformers

The go-to library for local embeddings. Load a pre-trained model, encode text, get vectors:

```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode(["Hello world", "Hi there"])
```

**Model choices:**
- `all-MiniLM-L6-v2` - Fast, 384 dimensions, great for prototyping
- `all-mpnet-base-v2` - Higher quality, 768 dimensions
- `multi-qa-MiniLM-L6-cos-v1` - Optimized for Q&A retrieval

### Domain Adaptation with TSDAE

TSDAE (Transformer Denoising Auto-Encoder) adapts embeddings to your domain *without labeled data*. It works by:

1. Taking your domain text
2. Corrupting it (deleting random words)
3. Training the model to reconstruct the original

The model learns your domain's vocabulary and relationships through this process.

When to use TSDAE:
- You have domain-specific terminology
- Off-the-shelf embeddings struggle on your data
- You have 10K+ unlabeled domain sentences
- You need better semantic search for your domain

### Evaluation Matters

"I fine-tuned the embeddings" means nothing without evaluation. You need to measure:

**Retrieval metrics:**
- MRR (Mean Reciprocal Rank) - Where does the first relevant result appear?
- Recall@K - How many relevant documents in top K?

**Similarity metrics:**
- Does cosine similarity correlate with human judgments?
- Do similar items cluster together?

Always compare before and after. Always use *your* data for evaluation.

### Bias Awareness

Embeddings encode biases from training data. "Doctor" might be closer to "man" than "woman". Job titles might carry gender or age associations.

This matters for fair retrieval. If you're building job search, biased embeddings could favor certain candidates over others for reasons unrelated to qualifications.

Mitigation:
- Measure for known biases
- Use balanced training data
- Consider post-processing debiasing
- Be transparent about limitations

## What's in This Module

| Script | What it shows |
|--------|---------------|
| <a href="../../modules/Phase%205%20-%20Custom%20Models%20and%20Multimodal/5.2-custom-embeddings/01_sentence_transformers.py">01_sentence_transformers.py</a> | Local embeddings with Sentence Transformers |
| <a href="../../modules/Phase%205%20-%20Custom%20Models%20and%20Multimodal/5.2-custom-embeddings/02_domain_adaptation_tsdae.py">02_domain_adaptation_tsdae.py</a> | Unsupervised adaptation for your domain |
| <a href="../../modules/Phase%205%20-%20Custom%20Models%20and%20Multimodal/5.2-custom-embeddings/03_embedding_evaluation.py">03_embedding_evaluation.py</a> | Measure embedding quality rigorously |
| <a href="../../modules/Phase%205%20-%20Custom%20Models%20and%20Multimodal/5.2-custom-embeddings/04_bias_in_embeddings.py">04_bias_in_embeddings.py</a> | Detect and mitigate embedding biases |

## Things to Think About

- **When do I switch from OpenAI to local?** When you need privacy, have scale (millions of embeddings), need low latency, or want domain customization. For small prototypes, OpenAI is simpler.

- **How much data for TSDAE?** 10K+ sentences recommended. More is better. Quality matters - domain-representative text, not random internet content.

- **Can I mix embedding models?** No. Embeddings from different models live in different vector spaces. If you change models, re-embed everything.

- **What about the max token limit?** Most models truncate at 512 tokens silently. For long documents, chunk first or use long-context embedding models.

## Related

- <a href="../phase-1-foundations/1.5-embeddings.md">Embeddings</a> - The basics
- <a href="../phase-1-foundations/1.6-vector-search.md">Vector Search</a> - Using embeddings for search
- <a href="../phase-3-advanced-patterns/3.4-advanced-rag.md">Advanced RAG</a> - Embeddings in RAG systems
- <a href="./5.1-fine-tuning.md">Fine-tuning</a> - When to fine-tune models instead

## Book References

- hands_on_LLM.I.2 - Embedding basics
- hands_on_LLM.III.10 - Custom embeddings and fine-tuning
- speach_lang.I.6.11 - Bias in word embeddings
