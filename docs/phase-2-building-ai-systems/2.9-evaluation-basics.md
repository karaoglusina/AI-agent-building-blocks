# Evaluation Basics

How do you know if your system is working? Not "does it produce output" but "does it produce good output?" Evaluation is how you answer this question systematically.

## Why This Matters

Without evaluation, you're flying blind. You make a change, it seems to work on a few examples, you ship it. Then it breaks in production. Systematic evaluation catches problems before users do.

For our job market analyzer, evaluation tells us: Are we retrieving the right jobs? Are classifications accurate? Are generated answers actually helpful?

## The Key Ideas

### Classification Metrics

For tasks that assign categories:

```python
from sklearn.metrics import precision_score, recall_score, f1_score

# Precision: Of what we predicted as positive, how many are actually positive?
# High precision = few false positives

# Recall: Of all actual positives, how many did we find?
# High recall = few false negatives

# F1: Harmonic mean of precision and recall
# Balances both concerns

precision = precision_score(y_true, y_pred, average='weighted')
recall = recall_score(y_true, y_pred, average='weighted')
f1 = f1_score(y_true, y_pred, average='weighted')
```

Use precision when false positives are costly. Use recall when false negatives are costly.

### Retrieval Metrics

For search and RAG:

```python
def precision_at_k(relevant, retrieved, k):
    """What fraction of top-k are relevant?"""
    top_k = retrieved[:k]
    return len(set(top_k) & set(relevant)) / k

def recall_at_k(relevant, retrieved, k):
    """What fraction of relevant docs are in top-k?"""
    top_k = retrieved[:k]
    return len(set(top_k) & set(relevant)) / len(relevant)

def mrr(relevant, retrieved):
    """How high is the first relevant result?"""
    for i, doc in enumerate(retrieved):
        if doc in relevant:
            return 1 / (i + 1)
    return 0
```

- **Precision@K**: Are the top results good?
- **Recall@K**: Did we find what we should have?
- **MRR**: How quickly do we find something relevant?

### Generation Metrics

For text generation:

```python
from rouge_score import rouge_scorer

scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'])
scores = scorer.score(reference_text, generated_text)

# ROUGE-1: Unigram overlap
# ROUGE-2: Bigram overlap
# ROUGE-L: Longest common subsequence
```

ROUGE and BLEU measure overlap with reference text. Useful but limited - they don't capture semantic equivalence.

### LLM as Judge

Use an LLM to evaluate outputs:

```python
class Evaluation(BaseModel):
    relevance: Literal["high", "medium", "low"]
    accuracy: Literal["correct", "partially_correct", "incorrect"]
    reasoning: str

response = client.beta.chat.completions.parse(
    model="gpt-4o",  # Use a capable model
    messages=[
        {"role": "system", "content": "Evaluate the following response..."},
        {"role": "user", "content": f"Query: {query}\nResponse: {response}\nReference: {reference}"}
    ],
    response_format=Evaluation
)
```

More nuanced than automatic metrics. Can evaluate things like helpfulness and coherence.

### The Eval Loop

Systematic evaluation:

```python
def run_evaluation(test_cases, system):
    results = []
    for case in test_cases:
        output = system(case["input"])
        score = evaluate(output, case["expected"])
        results.append({
            "input": case["input"],
            "output": output,
            "expected": case["expected"],
            "score": score
        })

    # Aggregate metrics
    return {
        "accuracy": mean([r["score"] for r in results]),
        "failures": [r for r in results if r["score"] < threshold]
    }
```

Run this after every change. Catch regressions early.

### A/B Comparison

When comparing approaches:

```python
def compare_systems(test_cases, system_a, system_b):
    for case in test_cases:
        output_a = system_a(case["input"])
        output_b = system_b(case["input"])

        # Which is better?
        comparison = compare_outputs(output_a, output_b, case["expected"])
        yield comparison
```

Direct comparison often reveals more than absolute metrics.

## What's in This Module

| Script | What it shows |
|--------|---------------|
| <a href="../../modules/Phase%202%20-%20Building%20AI%20Systems/2.9-evaluation-basics/01_classification_metrics.py">01_classification_metrics.py</a> | Precision, recall, F1, confusion matrix |
| <a href="../../modules/Phase%202%20-%20Building%20AI%20Systems/2.9-evaluation-basics/02_retrieval_metrics.py">02_retrieval_metrics.py</a> | MRR, precision@k, recall@k |
| <a href="../../modules/Phase%202%20-%20Building%20AI%20Systems/2.9-evaluation-basics/03_generation_metrics.py">03_generation_metrics.py</a> | BLEU, ROUGE |
| <a href="../../modules/Phase%202%20-%20Building%20AI%20Systems/2.9-evaluation-basics/04_llm_as_judge.py">04_llm_as_judge.py</a> | Using LLM for evaluation |
| <a href="../../modules/Phase%202%20-%20Building%20AI%20Systems/2.9-evaluation-basics/05_simple_eval_loop.py">05_simple_eval_loop.py</a> | Systematic evaluation pipeline |
| <a href="../../modules/Phase%202%20-%20Building%20AI%20Systems/2.9-evaluation-basics/06_comparison_eval.py">06_comparison_eval.py</a> | A/B comparison |
| <a href="../../modules/Phase%202%20-%20Building%20AI%20Systems/2.9-evaluation-basics/07_embedding_evaluation.py">07_embedding_evaluation.py</a> | Testing embeddings on your domain |

## Metric Selection

| Task | Primary Metrics | Secondary |
|------|-----------------|-----------|
| Classification | Precision, Recall, F1 | Confusion Matrix |
| Retrieval | MRR, Recall@K | NDCG, Precision@K |
| Generation | LLM-judge, ROUGE | BLEU |
| Ranking | NDCG, MRR | Precision@K |

## Best Practices

1. **Start with automated metrics** - Fast feedback
2. **Add LLM-as-judge** - More nuanced evaluation
3. **Include human eval** - Ground truth validation
4. **Track over time** - Catch regressions
5. **Test edge cases** - Where systems fail

## Things to Think About

- **How many test cases do you need?** Enough to be confident. 50-100 for quick checks, more for production decisions.
- **What makes a good test case?** Covers typical cases and edge cases. Has clear expected behavior.
- **When do metrics mislead?** ROUGE can be high with terrible output. Always sanity check with human review.

## Related

- <a href="./2.4-rag-pipeline.md">RAG Pipeline</a> - Evaluating retrieval
- <a href="./2.3-classification-routing.md">Classification & Routing</a> - Evaluating classification
- <a href="../phase-3-advanced-patterns/3.8-evaluation-systems.md">Evaluation Systems</a> - Production evaluation

## Book References

- AI_eng.3 - Evaluation fundamentals
- AI_eng.4 - Evaluation pipelines
- hands_on_LLM.III.12 - LLM evaluation
- speach_lang.I.4.7 - Classification metrics
