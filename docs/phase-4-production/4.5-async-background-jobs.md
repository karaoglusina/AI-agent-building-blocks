# Async & Background Jobs

Some operations are too slow for request-response. Processing 10,000 job postings, generating a long report, running evaluations - these need to happen in the background.

## Why This Matters

Users won't wait 5 minutes for a response. Background jobs let you accept a task, return immediately, and process asynchronously. Users can check status or get notified when done.

For our job market analyzer, background jobs handle bulk data imports, large-scale analysis, and any operation that takes more than a few seconds.

## The Key Ideas

### Asyncio Basics

Python's async lets you handle many concurrent operations:

```python
import asyncio

async def fetch_job(job_id: str):
    # Async operation
    response = await client.chat.completions.create(...)
    return response

async def process_all_jobs(job_ids: list[str]):
    # Run concurrently
    tasks = [fetch_job(id) for id in job_ids]
    results = await asyncio.gather(*tasks)
    return results
```

Good for I/O-bound concurrent operations.

### Concurrent LLM Calls

Multiple LLM calls in parallel:

```python
async def batch_classify(texts: list[str]) -> list[str]:
    async def classify_one(text):
        response = await async_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": f"Classify: {text}"}]
        )
        return response.choices[0].message.content

    results = await asyncio.gather(*[classify_one(t) for t in texts])
    return results
```

5x-10x faster than sequential.

### Celery for Background Jobs

For truly long-running tasks, use a task queue:

```python
from celery import Celery

app = Celery('tasks', broker='redis://localhost:6379/0')

@app.task
def process_document_batch(document_ids: list[str]):
    # This runs in a separate worker process
    for doc_id in document_ids:
        process_document(doc_id)
    return {"processed": len(document_ids)}
```

Tasks are queued and processed by workers.

### Task Status

Track job progress:

```python
from celery.result import AsyncResult

# Submit task
task = process_document_batch.delay(document_ids)
task_id = task.id

# Check status later
result = AsyncResult(task_id)
print(result.status)  # PENDING, STARTED, SUCCESS, FAILURE
print(result.result)  # Output when SUCCESS
```

### FastAPI + Background Tasks

For simpler cases, FastAPI has built-in background tasks:

```python
from fastapi import BackgroundTasks

@app.post("/process")
async def start_processing(background_tasks: BackgroundTasks):
    background_tasks.add_task(long_running_task, arg1, arg2)
    return {"status": "started"}
```

Good for fire-and-forget. Use Celery for complex workflows.

## What's in This Module

| Script | What it shows |
|--------|---------------|
| <a href="../../modules/Phase%204%20-%20Production%20Systems/4.5-async-background-jobs/01_asyncio_basics.py">01_asyncio_basics.py</a> | Async/await fundamentals |
| <a href="../../modules/Phase%204%20-%20Production%20Systems/4.5-async-background-jobs/02_concurrent_llm.py">02_concurrent_llm.py</a> | Parallel LLM calls |
| <a href="../../modules/Phase%204%20-%20Production%20Systems/4.5-async-background-jobs/03_celery_setup.py">03_celery_setup.py</a> | Configure Celery with Redis |
| <a href="../../modules/Phase%204%20-%20Production%20Systems/4.5-async-background-jobs/04_background_task.py">04_background_task.py</a> | Long-running tasks |
| <a href="../../modules/Phase%204%20-%20Production%20Systems/4.5-async-background-jobs/05_task_status.py">05_task_status.py</a> | Monitor job progress |

## Architecture

```
┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│  FastAPI    │ ──▶ │   Redis     │ ──▶ │   Celery    │
│  (API)      │     │  (Queue)    │     │  (Worker)   │
└─────────────┘     └─────────────┘     └─────────────┘
       │                                       │
       │           ┌─────────────┐            │
       └──────────▶│  PostgreSQL │◀───────────┘
                   │  (Results)  │
                   └─────────────┘
```

API accepts requests, queues tasks, workers process them.

## When to Use What

| Scenario | Approach |
|----------|----------|
| Multiple concurrent API calls | Asyncio |
| Long-running (>30s) | Celery |
| Fire-and-forget | FastAPI BackgroundTasks |
| Complex workflows | Celery with chains |
| High-throughput batch | Celery with batching |

## Celery with Docker

```yaml
services:
  app:
    build: .
    ports: ["8000:8000"]

  celery_worker:
    build: .
    command: celery -A tasks worker --loglevel=info

  redis:
    image: redis:alpine
```

Scale workers independently.

## Things to Think About

- **How do you handle failures?** Celery has retry mechanisms. Configure appropriately.
- **What about rate limits?** Add delays between calls or use Celery rate limiting.
- **How do users know when done?** Polling, webhooks, or websockets for notifications.

## Related

- <a href="../phase-3-advanced-patterns/3.6-fastapi-basics.md">FastAPI Basics</a> - API for triggering jobs
- <a href="./4.1-docker.md">Docker</a> - Containerizing workers
- <a href="../phase-3-advanced-patterns/3.5-iterative-processing.md">Iterative Processing</a> - Processing large content

## Book References

- AI_eng.9 - Async processing
- AI_eng.10 - Production infrastructure
