# MCP Servers

Model Context Protocol (MCP) is a standard for connecting LLMs to tools and data sources. Instead of building custom integrations, you use a common protocol that works across applications.

## Why This Matters

Every AI application needs tools - databases, APIs, file systems. MCP provides a standard way to expose these capabilities. Build once, use anywhere.

For our job market analyzer, MCP could expose the job database as a tool that any MCP-compatible client can query.

## The Key Ideas

### What is MCP?

MCP is a protocol for:
- **Tools**: Functions the LLM can call
- **Resources**: Data the LLM can access
- **Prompts**: Templates for common tasks

It standardizes how AI applications communicate with external capabilities.

### MCP Architecture

```
┌─────────────────┐       ┌─────────────────┐
│   AI Client     │ ◀───▶ │   MCP Server    │
│ (Claude, etc.)  │  MCP  │ (Your tools)    │
└─────────────────┘       └─────────────────┘
                                   │
                          ┌───────┴───────┐
                          │               │
                     ┌────▼────┐    ┌────▼────┐
                     │ Database│    │   API   │
                     └─────────┘    └─────────┘
```

### Using MCP Clients

Connect to existing MCP servers:

```python
from mcp import ClientSession

async with ClientSession() as session:
    # List available tools
    tools = await session.list_tools()

    # Call a tool
    result = await session.call_tool(
        name="search_jobs",
        arguments={"query": "ML engineer"}
    )
```

### Building MCP Servers

Expose your capabilities via MCP:

```python
from mcp.server import Server
from mcp.types import Tool, TextContent

server = Server("job-search")

@server.tool()
async def search_jobs(query: str, limit: int = 10):
    """Search for job postings."""
    results = await job_database.search(query, limit)
    return [TextContent(text=str(r)) for r in results]

# Run server
server.run()
```

Now any MCP client can search your job database.

## What's in This Module

| Script | What it shows |
|--------|---------------|
| [01_mcp_overview.py](../../modules/Phase 4 - Production Systems/4.6-mcp-servers/01_mcp_overview.py) | What MCP is and why |
| [02_mcp_client.py](../../modules/Phase 4 - Production Systems/4.6-mcp-servers/02_mcp_client.py) | Connect to MCP servers |
| [03_mcp_tool_use.py](../../modules/Phase 4 - Production Systems/4.6-mcp-servers/03_mcp_tool_use.py) | Call tools via MCP |
| [04_custom_mcp_server.py](../../modules/Phase 4 - Production Systems/4.6-mcp-servers/04_custom_mcp_server.py) | Build your own server |

## MCP vs Direct Integration

| Aspect | Direct Integration | MCP |
|--------|-------------------|-----|
| Setup | Custom per tool | Standard protocol |
| Reusability | Limited | Works everywhere |
| Maintenance | Per integration | Per server |
| Discovery | Manual | Protocol-based |

## Use Cases

### Database Access
```python
@server.tool()
async def query_database(sql: str):
    """Run SQL query."""
    return await db.execute(sql)
```

### API Integration
```python
@server.tool()
async def fetch_weather(location: str):
    """Get weather for location."""
    return await weather_api.get(location)
```

### File System
```python
@server.tool()
async def read_file(path: str):
    """Read file contents."""
    return await aiofiles.open(path).read()
```

## Things to Think About

- **When should you use MCP?** When building tools that should work with multiple AI clients, or when connecting to existing MCP services.
- **Security considerations?** MCP servers can expose sensitive operations. Implement proper authentication and authorization.
- **What about latency?** MCP adds a layer. For performance-critical paths, direct integration might be better.

## Related

- [Agent Orchestration](../phase-2-building-ai-systems/2.5-agent-orchestration.md) - Tools in agents
- [FastAPI Basics](../phase-3-advanced-patterns/3.6-fastapi-basics.md) - Alternative: HTTP APIs

## Book References

- AI_eng.6 - Tool integration patterns
