# Iterative Processing

Some content is too large for one LLM call. A 100-page document, 10,000 job postings, a book-length analysis. Iterative processing breaks large tasks into manageable pieces.

## Why This Matters

LLMs have context limits. Even with 128k tokens, you can't fit everything. And even when you can, quality degrades with very long contexts. Iterative processing handles arbitrarily large content.

For our job market analyzer, this means analyzing trends across all 10,000 job postings, creating executive summaries from hundreds of sources, and refining outputs to publication quality.

## The Key Ideas

### Map-Reduce

Divide, process independently, combine:

```
Large Document → [Chunk 1, Chunk 2, ..., Chunk N]
              → Process each chunk (MAP)
              → Combine results (REDUCE)

# Map: "Summarize this chunk"
# Reduce: "Combine these summaries into one"
```

Handles documents of any size by dividing work.

### Progressive Summarization

Summarize in stages for better quality:

```
Document → Paragraph summaries
        → Section summaries (combine paragraphs)
        → Document summary (combine sections)
```

Each level builds on the previous. Better than trying to summarize a massive document directly.

### Refinement Chain

Generate, critique, improve:

```
Initial output → Critique: "Missing salary data"
              → Refined v2: Added salary info
              → Critique: "Needs more examples"
              → Refined v3: Added examples
              → Good enough? Return
```

Iterative improvement converges to high quality.

### Hierarchical Processing

Process at multiple levels of abstraction:

```
Level 1: Process individual jobs (extract skills)
Level 2: Group by category (frontend, backend, ML)
Level 3: Create market overview (trends, insights)
```

Works from detail to abstraction.

### Batch Processing

Handle thousands of items efficiently:

```python
async def process_batch(items: list, batch_size: int = 50):
    batches = [items[i:i+batch_size] for i in range(0, len(items), batch_size)]

    results = []
    for batch in batches:
        # Process batch concurrently
        batch_results = await asyncio.gather(
            *[process_item(item) for item in batch]
        )
        results.extend(batch_results)
        await asyncio.sleep(1)  # Rate limiting

    return results
```

Concurrent processing with rate limiting for throughput.

## What's in This Module

| Script | What it shows |
|--------|---------------|
| <a href="../../modules/Phase%203%20-%20Advanced%20Patterns/3.5-iterative-processing/01_map_reduce.py">01_map_reduce.py</a> | Divide, process, combine |
| <a href="../../modules/Phase%203%20-%20Advanced%20Patterns/3.5-iterative-processing/02_progressive_summary.py">02_progressive_summary.py</a> | Multi-stage summarization |
| <a href="../../modules/Phase%203%20-%20Advanced%20Patterns/3.5-iterative-processing/03_refinement_chain.py">03_refinement_chain.py</a> | Iterative improvement |
| <a href="../../modules/Phase%203%20-%20Advanced%20Patterns/3.5-iterative-processing/04_hierarchical_processing.py">04_hierarchical_processing.py</a> | Detail to abstraction |
| <a href="../../modules/Phase%203%20-%20Advanced%20Patterns/3.5-iterative-processing/05_batch_processing.py">05_batch_processing.py</a> | High-throughput processing |

## Pattern Selection

| Content Type | Best Pattern |
|--------------|--------------|
| Very large documents | Map-Reduce |
| Long docs needing quality | Progressive Summary |
| Quality-critical output | Refinement Chain |
| Multi-level insights | Hierarchical |
| Many small items | Batch Processing |

## Performance Considerations

| Pattern | Latency | Cost | Quality |
|---------|---------|------|---------|
| Map-Reduce | High (N chunks) | Linear | Good |
| Progressive | Medium (3-5 passes) | 2-3x direct | High |
| Refinement | High (2-4 iterations) | 2-4x | Highest |
| Hierarchical | High (multiple levels) | Variable | High |
| Batch | Depends on parallelism | Per-item | Same |

## Combining Patterns

Patterns work together:

```python
# Process 10k job postings:

# 1. Batch process: Extract key info from each
job_summaries = await batch_process(jobs, extract_info)

# 2. Hierarchical: Group by category
by_category = group_by_category(job_summaries)

# 3. Map-reduce each category
category_insights = []
for category, items in by_category.items():
    insight = map_reduce(items, summarize_chunk, combine_summaries)
    category_insights.append(insight)

# 4. Progressive: Create executive summary
executive_summary = progressive_summarize(category_insights)

# 5. Refinement: Polish the output
final = refine_until_good(executive_summary)
```

## Things to Think About

- **When does map-reduce lose information?** When important context spans chunks. Overlap helps, but some loss is inevitable.
- **How many refinement iterations?** Usually 2-4. Diminishing returns after that.
- **What's the optimal batch size?** Balance throughput vs rate limits. 50-100 is often good.

## Related

- <a href="../phase-2-building-ai-systems/2.6-context-engineering.md">Context Engineering</a> - Managing what fits in context
- <a href="./3.9-document-processing.md">Document Processing</a> - Preparing documents for processing
- <a href="./3.8-evaluation-systems.md">Evaluation Systems</a> - Measuring processing quality

## Book References

- AI_eng.6 - Large document handling
- AI_eng.9 - Async processing
- NLP_cook.9 - Summarization techniques
