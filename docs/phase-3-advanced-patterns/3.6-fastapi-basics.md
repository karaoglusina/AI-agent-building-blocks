# FastAPI Basics

At some point, your AI system needs to be accessible. FastAPI makes it easy to build production-ready APIs with automatic validation, async support, and interactive documentation.

## Why This Matters

Scripts are great for learning. APIs are great for using. FastAPI bridges the gap - it's fast, type-safe, and handles the HTTP plumbing so you can focus on the AI.

For our job market analyzer, FastAPI means anyone can query your system, not just people who can run Python scripts.

## The Key Ideas

### Basic Setup

```python
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

class SearchRequest(BaseModel):
    query: str
    max_results: int = 10

@app.post("/search")
def search(request: SearchRequest):
    results = search_jobs(request.query, request.max_results)
    return {"results": results}
```

That's a working API. Pydantic validates inputs automatically. Visit `/docs` for interactive Swagger UI.

### Async Endpoints

LLM calls are I/O-bound. Async handles them efficiently:

```python
@app.post("/chat")
async def chat(request: ChatRequest):
    response = await client.chat.completions.create(
        model="gpt-4o-mini",
        messages=request.messages
    )
    return {"response": response.choices[0].message.content}
```

Multiple concurrent requests don't block each other.

### Streaming Responses

For long generations, stream tokens:

```python
from fastapi.responses import StreamingResponse

@app.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    async def generate():
        stream = await client.chat.completions.create(
            model="gpt-4o-mini",
            messages=request.messages,
            stream=True
        )
        async for chunk in stream:
            if chunk.choices[0].delta.content:
                yield f"data: {chunk.choices[0].delta.content}\n\n"
        yield "data: [DONE]\n\n"

    return StreamingResponse(generate(), media_type="text/event-stream")
```

Users see progress instead of waiting.

### Chat Endpoint

Maintain conversation state:

```python
conversations = {}  # In production, use a database

@app.post("/chat")
async def chat(request: ChatRequest):
    # Get or create conversation
    conv_id = request.conversation_id or str(uuid4())
    if conv_id not in conversations:
        conversations[conv_id] = []

    # Add user message
    conversations[conv_id].append({"role": "user", "content": request.message})

    # Generate response
    response = await client.chat.completions.create(
        model="gpt-4o-mini",
        messages=conversations[conv_id]
    )

    # Add assistant response
    assistant_message = response.choices[0].message.content
    conversations[conv_id].append({"role": "assistant", "content": assistant_message})

    return {"conversation_id": conv_id, "response": assistant_message}
```

### RAG Endpoint

Full RAG pipeline as an API:

```python
@app.post("/rag")
async def rag_query(request: RAGRequest):
    # Retrieve
    results = collection.query(
        query_texts=[request.query],
        n_results=request.n_results
    )

    # Augment
    context = format_context(results)

    # Generate
    response = await client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": f"Context:\n{context}"},
            {"role": "user", "content": request.query}
        ]
    )

    return {
        "answer": response.choices[0].message.content,
        "sources": results["ids"][0]
    }
```

## What's in This Module

| Script | What it shows |
|--------|---------------|
| [01_hello_fastapi.py](../../modules/Phase 3 - Advanced Patterns/3.6-fastapi-basics/01_hello_fastapi.py) | Minimal API setup |
| [02_pydantic_validation.py](../../modules/Phase 3 - Advanced Patterns/3.6-fastapi-basics/02_pydantic_validation.py) | Request/response validation |
| [03_async_endpoints.py](../../modules/Phase 3 - Advanced Patterns/3.6-fastapi-basics/03_async_endpoints.py) | Non-blocking endpoints |
| [04_streaming_response.py](../../modules/Phase 3 - Advanced Patterns/3.6-fastapi-basics/04_streaming_response.py) | Stream LLM output |
| [05_chat_endpoint.py](../../modules/Phase 3 - Advanced Patterns/3.6-fastapi-basics/05_chat_endpoint.py) | Stateful conversation |
| [06_rag_endpoint.py](../../modules/Phase 3 - Advanced Patterns/3.6-fastapi-basics/06_rag_endpoint.py) | Full RAG pipeline |

## Running FastAPI

```bash
# Run the server
python 01_hello_fastapi.py
# Or: uvicorn 01_hello_fastapi:app --reload

# Visit http://localhost:8000/docs for Swagger UI
```

Test with curl:
```bash
curl -X POST "http://localhost:8000/search" \
  -H "Content-Type: application/json" \
  -d '{"query": "ML engineer", "max_results": 5}'
```

## Production Considerations

### Error Handling
```python
from fastapi import HTTPException

@app.post("/search")
async def search(request: SearchRequest):
    try:
        return search_jobs(request.query)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
```

### CORS (for frontend)
```python
from fastapi.middleware.cors import CORSMiddleware

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)
```

### Environment Config
```python
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    openai_api_key: str
    class Config:
        env_file = ".env"

settings = Settings()
```

## Things to Think About

- **How do you handle long-running requests?** Streaming for user-facing, background tasks for processing.
- **What about authentication?** FastAPI supports OAuth2, API keys, JWT - add what your use case needs.
- **How do you scale?** Run multiple workers with Gunicorn/Uvicorn, use a load balancer.

## Related

- [Async & Background Jobs](../phase-4-production/4.5-async-background-jobs.md) - Long-running tasks
- [Docker](../phase-4-production/4.1-docker.md) - Containerizing your API
- [Cloud Deployment](../phase-4-production/4.7-cloud-deployment.md) - Deploying to production

## Book References

- AI_eng.2 - Pydantic validation
- AI_eng.9 - Async processing
