# Document Processing

Most enterprise knowledge is locked in PDFs. Technical docs, research papers, reports, contracts. To use this knowledge in AI systems, you need to extract it, preserve structure, and prepare it for indexing.

## Why This Matters

You can't do RAG over a PDF. You need to extract the text, chunk it appropriately, add metadata, and index it. Document processing is the bridge between static files and searchable knowledge.

For our job market analyzer, this means processing job description PDFs, company reports, and industry analyses - turning documents into searchable content.

## The Key Ideas

### PyMuPDF Extraction

PyMuPDF (fitz) is fast and accurate:

```python
import fitz  # PyMuPDF

doc = fitz.open("document.pdf")
text = ""
for page in doc:
    text += page.get_text()
```

Best for production use. Handles complex PDFs well.

### pypdf Extraction

Pure Python, easy installation:

```python
from pypdf import PdfReader

reader = PdfReader("document.pdf")
text = ""
for page in reader.pages:
    text += page.extract_text()
```

Simpler but less robust. Good for prototyping.

### Structure Preservation

Extract more than just text:

```python
# Get text with layout info
blocks = page.get_text("dict")

# Detect headings by font size
for block in blocks["blocks"]:
    if "lines" in block:
        for line in block["lines"]:
            for span in line["spans"]:
                if span["size"] > 14:  # Larger font = heading
                    print(f"HEADING: {span['text']}")
```

Preserving headings and sections improves chunking.

### Chunking Strategy

After extraction, chunk appropriately:

```python
def chunk_document(text, chunk_size=500, overlap=50):
    # Simple fixed-size chunking
    chunks = []
    for i in range(0, len(text), chunk_size - overlap):
        chunk = text[i:i + chunk_size]
        chunks.append({
            "text": chunk,
            "start": i,
            "end": i + len(chunk)
        })
    return chunks
```

Better: chunk by section boundaries when structure is available.

### Full Pipeline

From PDF to searchable index:

```python
def process_pdf(pdf_path: str, collection):
    # 1. Extract
    doc = fitz.open(pdf_path)
    pages = [{"number": i+1, "text": page.get_text()} for i, page in enumerate(doc)]

    # 2. Chunk with metadata
    all_chunks = []
    for page in pages:
        chunks = chunk_text(page["text"])
        for i, chunk in enumerate(chunks):
            all_chunks.append({
                "text": chunk,
                "source": pdf_path,
                "page": page["number"],
                "chunk_index": i
            })

    # 3. Index
    collection.add(
        documents=[c["text"] for c in all_chunks],
        ids=[f"{pdf_path}_{c['page']}_{c['chunk_index']}" for c in all_chunks],
        metadatas=[{"source": c["source"], "page": c["page"]} for c in all_chunks]
    )
```

## What's in This Module

| Script | What it shows |
|--------|---------------|
| <a href="../../modules/Phase%203%20-%20Advanced%20Patterns/3.9-document-processing/01_pdf_pymupdf.py">01_pdf_pymupdf.py</a> | Fast, accurate extraction |
| <a href="../../modules/Phase%203%20-%20Advanced%20Patterns/3.9-document-processing/02_pdf_pypdf.py">02_pdf_pypdf.py</a> | Pure Python extraction |
| <a href="../../modules/Phase%203%20-%20Advanced%20Patterns/3.9-document-processing/03_pdf_with_structure.py">03_pdf_with_structure.py</a> | Preserve headings and hierarchy |
| <a href="../../modules/Phase%203%20-%20Advanced%20Patterns/3.9-document-processing/04_pdf_to_chunks.py">04_pdf_to_chunks.py</a> | Full pipeline to indexed chunks |

## Library Comparison

| Library | Speed | Accuracy | Complex PDFs | Installation |
|---------|-------|----------|--------------|--------------|
| PyMuPDF | Fast | Excellent | Good | Needs C bindings |
| pypdf | Moderate | Good | Basic | Pure Python |

Use PyMuPDF for production. Use pypdf for quick prototyping.

## Common Challenges

### Scanned PDFs (Images)
```python
# Use OCR
import pytesseract
from PIL import Image

text = pytesseract.image_to_string(image)
```

### Tables
```python
# Use specialized library
import camelot
tables = camelot.read_pdf("file.pdf")
```

### Multi-Column Layouts
```python
# Sort blocks by position
blocks = page.get_text("blocks")
sorted_blocks = sorted(blocks, key=lambda b: (b[1], b[0]))
```

## Metadata to Preserve

Essential:
- **Page number**: For citations
- **Source file**: Original document
- **Chunk ID**: Unique identifier

Optional:
- **Section/heading**: Context
- **Author/date**: Document metadata
- **Document type**: Category

## Things to Think About

- **What about protected PDFs?** Some PDFs are encrypted or have copy protection. Handle gracefully.
- **How do you handle extraction errors?** Some PDFs are malformed. Log errors, skip problematic files, continue processing.
- **What's the right chunk size?** Balance between too small (no context) and too large (imprecise retrieval). 200-500 tokens typically works.

## Related

- <a href="../phase-2-building-ai-systems/2.1-text-preparation.md">Text Preparation</a> - Chunking strategies
- <a href="../phase-2-building-ai-systems/2.4-rag-pipeline.md">RAG Pipeline</a> - Using processed documents
- <a href="../phase-1-foundations/1.6-vector-search.md">Vector Search</a> - Indexing chunks

## Book References

- AI_eng.6 - RAG and chunking
- AI_eng.8 - Data processing
