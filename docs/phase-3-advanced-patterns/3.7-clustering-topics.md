# Clustering & Topics

What patterns exist in 10,000 job postings? What are the main job categories? What topics emerge? Clustering and topic modeling answer these questions without manual labeling.

## Why This Matters

Manual categorization doesn't scale. Clustering finds structure automatically. You might discover segments you didn't know existed - "fintech ML roles that require blockchain experience" as a distinct cluster.

For our job market analyzer, clustering reveals market structure: what categories exist, how they relate, what makes each distinctive.

## The Key Ideas

### K-Means Clustering

Group documents by embedding similarity:

```python
from sklearn.cluster import KMeans

# Embed documents
embeddings = embed_documents(job_descriptions)

# Cluster
kmeans = KMeans(n_clusters=10)
labels = kmeans.fit_predict(embeddings)

# Now each job has a cluster label 0-9
```

Simple, fast, well-understood. You choose K (number of clusters).

### UMAP Visualization

High-dimensional embeddings are hard to visualize. UMAP reduces them to 2D:

```python
import umap

reducer = umap.UMAP(n_components=2)
coords_2d = reducer.fit_transform(embeddings)

# Plot with matplotlib
plt.scatter(coords_2d[:, 0], coords_2d[:, 1], c=labels)
```

See cluster structure visually. Catch problems like overlapping clusters.

### BERTopic

End-to-end topic modeling:

```python
from bertopic import BERTopic

model = BERTopic()
topics, probs = model.fit_transform(documents)

# Get topic info
model.get_topic_info()
# Topic 0: ['machine', 'learning', 'python', 'tensorflow']
# Topic 1: ['frontend', 'react', 'javascript', 'css']
```

Automatically discovers topics with interpretable keywords. No need to specify number of topics.

### LLM Cluster Labeling

Cluster keywords aren't always clear. Use an LLM to generate human-readable labels:

```python
def label_cluster(sample_docs: list[str]) -> str:
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{
            "role": "user",
            "content": f"These job postings belong to one cluster. "
                       f"Give a short, descriptive label:\n{sample_docs}"
        }]
    )
    return response.choices[0].message.content
    # "Senior Backend Engineering Roles"
```

Turns "Cluster 3" into something meaningful.

### Topic Coherence

Evaluate topic quality:

```python
def coherence_score(topic_words: list[str], embeddings_model) -> float:
    """Higher = more coherent topic"""
    word_embeddings = [embeddings_model.encode(w) for w in topic_words]
    # Average pairwise similarity
    similarities = []
    for i, e1 in enumerate(word_embeddings):
        for e2 in word_embeddings[i+1:]:
            similarities.append(cosine_similarity(e1, e2))
    return np.mean(similarities)
```

Good topics have coherent, related words. Low coherence suggests the clustering needs tuning.

## What's in This Module

| Script | What it shows |
|--------|---------------|
| [01_kmeans_clustering.py](../../modules/Phase%203%20-%20Advanced%20Patterns/3.7-clustering-topics/01_kmeans_clustering.py) | Basic document clustering |
| [02_umap_visualization.py](../../modules/Phase%203%20-%20Advanced%20Patterns/3.7-clustering-topics/02_umap_visualization.py) | Visualize embeddings in 2D |
| [03_bertopic_basics.py](../../modules/Phase%203%20-%20Advanced%20Patterns/3.7-clustering-topics/03_bertopic_basics.py) | Automatic topic discovery |
| [04_cluster_labeling.py](../../modules/Phase%203%20-%20Advanced%20Patterns/3.7-clustering-topics/04_cluster_labeling.py) | Generate readable labels |
| [05_topic_coherence.py](../../modules/Phase%203%20-%20Advanced%20Patterns/3.7-clustering-topics/05_topic_coherence.py) | Evaluate topic quality |
| [06_interactive_exploration.py](../../modules/Phase%203%20-%20Advanced%20Patterns/3.7-clustering-topics/06_interactive_exploration.py) | Programmatic cluster exploration |

## Technique Selection

| Goal | Approach |
|------|----------|
| Quick exploration | K-Means + UMAP |
| Topic discovery | BERTopic |
| Known number of categories | K-Means |
| Unknown number | BERTopic (uses HDBSCAN) |
| Human-readable names | LLM labeling |
| Quality assessment | Coherence metrics |

## Job Market Applications

- **Market segmentation**: Entry-level vs Senior vs Lead
- **Skill clustering**: Frontend vs Backend vs Full-stack
- **Industry sectors**: Fintech vs Healthtech vs E-commerce
- **Compensation bands**: Cluster by salary patterns
- **Location patterns**: Remote vs Onsite vs Hybrid

## Parameter Tuning

### K-Means
- **n_clusters**: Use elbow method to find optimal K
- Start with sqrt(N/2) as a rough estimate

### UMAP
- **n_neighbors**: 5-50, higher preserves more global structure
- **min_dist**: 0.0-1.0, lower creates tighter clusters

### BERTopic
- **min_topic_size**: 10-50, higher means fewer larger topics
- **nr_topics**: None for auto, or set specific number

## Things to Think About

- **How many clusters?** Use elbow method or BERTopic's automatic detection. Too many = noise, too few = oversimplification.
- **What if clusters overlap?** Some documents are genuinely between categories. Soft clustering or multiple labels might be appropriate.
- **How do you validate clusters?** Check coherence, review samples manually, see if they match domain intuition.

## Related

- [Embeddings](../phase-1-foundations/1.5-embeddings.md) - Foundation for clustering
- [Information Extraction](../phase-2-building-ai-systems/2.2-information-extraction.md) - Extract features for clustering
- [Evaluation Systems](./3.8-evaluation-systems.md) - Measuring cluster quality

## Book References

- hands_on_LLM.II.5 - Clustering with transformers
- NLP_cook.4 - Text clustering
- NLP_cook.6 - Topic modeling
