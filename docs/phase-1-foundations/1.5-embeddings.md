# Embeddings

When I first tried to build a job search feature, I used keyword matching. Search for "machine learning" and get ML jobs. But that misses jobs that say "deep learning" or "neural networks" or "AI researcher." Same meaning, different words.

Embeddings solve this. They represent meaning, not just words.

## Why This Matters

Embeddings are the foundation of semantic search, which is the foundation of RAG, which is probably 80% of what you'll build. Understanding embeddings means understanding how to find relevant content in a sea of data.

For our job market analyzer, embeddings let us find jobs similar to one you like, even if they use completely different words to describe the same skills.

## The Key Ideas

### Text â†’ Vector

An embedding model takes text and outputs a vector - a list of numbers (typically 1536 or 3072 of them):

```python
from openai import OpenAI
client = OpenAI()

response = client.embeddings.create(
    model="text-embedding-3-small",
    input="machine learning engineer"
)

embedding = response.data[0].embedding  # [0.023, -0.041, 0.112, ...]
```

These numbers encode meaning. You can't interpret them directly, but similar meanings produce similar vectors.

### Similarity is Distance

"Machine learning engineer" and "ML engineer" have nearly identical embeddings. "Chef" has a completely different one. You measure this with cosine similarity:

```python
import numpy as np

def cosine_similarity(a, b):
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

# Returns 0-1, where 1 = identical meaning
similarity = cosine_similarity(embedding1, embedding2)
```

This is the core primitive for semantic search.

### The Search Algorithm

```python
# Once, upfront: embed all your documents
documents = ["job posting 1...", "job posting 2...", ...]
doc_embeddings = [embed(doc) for doc in documents]

# At query time: embed the query and find similar docs
query_embedding = embed("remote ML jobs in Europe")
similarities = [cosine_similarity(query_embedding, doc) for doc in doc_embeddings]
top_results = sorted(zip(documents, similarities), key=lambda x: -x[1])[:10]
```

That's semantic search in 5 lines.

### Batch Your Requests

Don't embed one document at a time:

```python
# Slow and expensive
embeddings = [client.embeddings.create(input=doc) for doc in documents]

# Fast and cheap
response = client.embeddings.create(input=documents)  # Send batch
embeddings = [d.embedding for d in response.data]
```

OpenAI charges per token, not per request. Batching is just faster.

### Models

- **text-embedding-3-small**: Best for most use cases. 1536 dimensions.
- **text-embedding-3-large**: Higher quality, more dimensions (3072). Use if you need maximum accuracy.

The "small" model is good enough for almost everything.

## What's in This Module

| Script | What it shows |
|--------|---------------|
| [01_basic_embedding.py](../../modules/Phase%201%20-%20Foundations/1.5-embeddings/01_basic_embedding.py) | Create a single embedding |
| [02_embedding_models.py](../../modules/Phase%201%20-%20Foundations/1.5-embeddings/02_embedding_models.py) | Compare different embedding models |
| [03_batch_embeddings.py](../../modules/Phase%201%20-%20Foundations/1.5-embeddings/03_batch_embeddings.py) | Embed multiple texts efficiently |
| [04_cosine_similarity.py](../../modules/Phase%201%20-%20Foundations/1.5-embeddings/04_cosine_similarity.py) | Measure vector similarity |
| [05_simple_search.py](../../modules/Phase%201%20-%20Foundations/1.5-embeddings/05_simple_search.py) | Basic vector search implementation |
| [06_job_search.py](../../modules/Phase%201%20-%20Foundations/1.5-embeddings/06_job_search.py) | Search the job data by meaning |
| [07_save_load_embeddings.py](../../modules/Phase%201%20-%20Foundations/1.5-embeddings/07_save_load_embeddings.py) | Persist embeddings to disk |
| [08_numpy_operations.py](../../modules/Phase%201%20-%20Foundations/1.5-embeddings/08_numpy_operations.py) | Essential numpy for vector operations |

## Things to Think About

- **What happens when you embed very long text?** The model truncates it. For long documents, chunk them first and embed each chunk.
- **Do embeddings work across languages?** OpenAI's models have some multilingual capability, but they're strongest in English.
- **Why not just use the LLM to compare similarity?** Too slow and expensive. Embeddings let you compare thousands of documents in milliseconds.

## Related

- [Vector Search](./1.6-vector-search.md) - Using a database for embeddings at scale
- [RAG Pipeline](../phase-2-building-ai-systems/2.4-rag-pipeline.md) - Embeddings in retrieval-augmented generation
- [Text Preparation](../phase-2-building-ai-systems/2.1-text-preparation.md) - Preparing text before embedding

## Book References

- hands_on_LLM.I.2 - Embedding fundamentals
- speach_lang.I.6 - Vector semantics
