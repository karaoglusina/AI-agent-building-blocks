# Advanced RAG

Basic RAG retrieves documents and generates answers. Advanced RAG rewrites queries for better retrieval, combines keyword and semantic search, reranks results for precision, and handles questions requiring multiple retrieval steps.

## Why This Matters

Basic RAG fails in predictable ways. User asks a vague question, retrieval returns irrelevant results, answer is wrong. Advanced techniques address these failure modes.

For our job market analyzer, advanced RAG means better handling of complex queries like "What Python jobs in fintech offer similar compensation to the FAANG roles we discussed yesterday?"

## Scripts in This Module

| Script | What it shows |
|--------|---------------|
| <a href="../../scripts/phase-3-advanced-patterns/3.4-advanced-rag/01_query_rewriting.py">01_query_rewriting.py</a> | Transform queries for better retrieval |
| <a href="../../scripts/phase-3-advanced-patterns/3.4-advanced-rag/02_multi_hop_rag.py">02_multi_hop_rag.py</a> | Multiple retrieval steps |
| <a href="../../scripts/phase-3-advanced-patterns/3.4-advanced-rag/03_self_rag.py">03_self_rag.py</a> | Decide when to retrieve |
| <a href="../../scripts/phase-3-advanced-patterns/3.4-advanced-rag/04_cross_encoder_rerank.py">04_cross_encoder_rerank.py</a> | Two-stage retrieval |
| <a href="../../scripts/phase-3-advanced-patterns/3.4-advanced-rag/05_hybrid_search.py">05_hybrid_search.py</a> | BM25 + semantic search |
| <a href="../../scripts/phase-3-advanced-patterns/3.4-advanced-rag/06_rag_fusion.py">06_rag_fusion.py</a> | Multiple query perspectives |

## The Key Ideas

### Query Rewriting

User queries are often vague or conversational. Rewrite for better retrieval:

```python
# Original: "what about remote ones?"
# Context: Previous query was about ML jobs

rewritten = rewrite_query(
    query="what about remote ones?",
    context="user was searching for ML engineer jobs"
)
# → "remote ML engineer jobs"
```

Also useful for query expansion:
- "ML jobs" → "machine learning engineer deep learning AI jobs"

### Multi-Hop RAG

Some questions need multiple retrieval steps:

```
Q: "How do salaries at companies with offices in Amsterdam compare to
    those with offices in Berlin?"

Step 1: Retrieve companies with Amsterdam offices
Step 2: Retrieve companies with Berlin offices
Step 3: Retrieve salary data for each
Step 4: Compare and synthesize
```

The agent decides what to retrieve at each step based on what it learned.

### Self-RAG

Sometimes retrieval isn't needed:

```python
def should_retrieve(query: str, model_knowledge: bool) -> bool:
    """Decide if retrieval is necessary"""
    if is_factual_question(query):
        return True
    if is_conversational(query):
        return False
    if model_can_answer(query):
        return False
    return True
```

Skip retrieval for simple conversational turns. Retrieve for factual questions.

### Cross-Encoder Reranking

Bi-encoder search is fast but imprecise. Cross-encoder reranking improves precision:

```python
# Fast retrieval: Get top 50 candidates
candidates = vector_search(query, k=50)

# Precise reranking: Score each with cross-encoder
reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")
scores = reranker.predict([(query, doc) for doc in candidates])

# Return top 5 after reranking
top_results = sorted(zip(candidates, scores), key=lambda x: -x[1])[:5]
```

Two-stage retrieval: fast recall, then precise ranking.

### Hybrid Search

Combine semantic search with keyword matching:

```python
# Semantic: Good for meaning, bad for exact terms
semantic_results = vector_search(query)

# Keyword: Good for exact terms, bad for meaning
keyword_results = bm25_search(query)

# Combine: Get the best of both
combined = reciprocal_rank_fusion(semantic_results, keyword_results)
```

"Python developer" matches "Python" exactly (keyword) and "software engineer" semantically (vector).

### RAG Fusion

Multiple query perspectives improve recall:

```python
# Generate query variations
queries = [
    original_query,
    rephrase(original_query),
    expand_query(original_query),
    generate_subquery(original_query)
]

# Search with each
all_results = [search(q) for q in queries]

# Combine with reciprocal rank fusion
final = reciprocal_rank_fusion(all_results)
```

Different phrasings retrieve different relevant documents.

## Technique Selection

| Problem | Solution |
|---------|----------|
| Vague queries | Query rewriting |
| Complex questions | Multi-hop RAG |
| Over-retrieval | Self-RAG |
| Imprecise ranking | Cross-encoder reranking |
| Missing exact matches | Hybrid search |
| Incomplete recall | RAG fusion |

## Performance vs Quality

| Technique | Latency | Quality Improvement |
|-----------|---------|---------------------|
| Query rewriting | +1 LLM call | Moderate |
| Multi-hop | +N retrievals | High for complex Q |
| Self-RAG | +1 decision | Variable |
| Reranking | +50-100ms | High |
| Hybrid | +keyword search | Moderate |
| RAG fusion | +N searches | High |

## Things to Think About

- **When is advanced RAG worth it?** When basic RAG fails regularly. Start simple, add complexity when needed.
- **How do you evaluate RAG quality?** Retrieval metrics (recall@k, MRR) separately from generation quality.
- **What if reranking disagrees with retrieval?** Trust the reranker - it sees query and document together.

## Related

- [RAG Pipeline](../phase-2-building-ai-systems/2.4-rag-pipeline.md) - Basic RAG patterns
- [Vector Search](../phase-1-foundations/1.6-vector-search.md) - Foundation for retrieval
- [Evaluation Systems](./3.8-evaluation-systems.md) - Measuring RAG quality

## Book References

- AI_eng.6 - RAG architecture
- hands_on_LLM.II.8 - Retrieval systems
