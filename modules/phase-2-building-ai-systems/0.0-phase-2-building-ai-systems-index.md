# Phase 2: Building AI Systems

This is where it gets interesting. Phase 1 gave you the building blocks. Phase 2 shows you how to assemble them into systems that actually do things.

## What You're Building Toward

By the end of Phase 2, you'll be able to build a real AI application - something like our job market analyzer that can:
- Process and clean messy text data
- Extract structured information from job postings
- Classify and route queries to the right handlers
- Answer questions using RAG over your data
- Use tools and make decisions like an agent
- Remember context across conversations
- Evaluate whether your system is actually working

This is the 80/20 phase. These patterns appear in almost every AI application.

## The Modules

### <a href="./2.1-text-preparation.md">Text Preparation</a>
Before you can do anything useful with text, you need to clean it and chunk it. This module covers tokenization, normalization, and chunking strategies. Not glamorous, but essential.

### <a href="./2.2-information-extraction.md">Information Extraction</a>
How do you pull structured data from unstructured text? NER, keyword extraction, regex patterns, and LLM-based extraction. Multiple tools for different situations.

### <a href="./2.3-classification-routing.md">Classification & Routing</a>
Categorize text and route it to the right handler. Zero-shot, few-shot, intent detection. This is how you build systems that respond appropriately to different inputs.

### <a href="./2.4-rag-pipeline.md">RAG Pipeline</a>
The most important pattern. Retrieve relevant context, augment the prompt, generate a grounded response. This is how you make LLMs useful for your specific data.

### <a href="./2.5-agent-orchestration.md">Agent Orchestration</a>
Connect the LLM to tools and let it decide what to do. The core agent loop, error handling, multi-tool agents. This is where systems become autonomous.

### <a href="./2.6-context-engineering.md">Context Engineering</a>
Managing what goes into the limited context window. Prioritization, compression, dynamic prompts. Critical when you're working with lots of data.

### <a href="./2.7-memory-patterns.md">Memory Patterns</a>
How do agents remember? Conversation buffers, sliding windows, summarization, long-term storage. Essential for any conversational system.

### <a href="./2.8-prompt-engineering.md">Prompt Engineering</a>
Getting better outputs through better prompts. System design, chain of thought, few-shot examples, defensive prompting.

### <a href="./2.9-evaluation-basics.md">Evaluation Basics</a>
How do you know if it's working? Classification metrics, retrieval metrics, LLM-as-judge. You can't improve what you don't measure.

## The Flow

These modules can be approached in different orders, but here's a suggested path:

```
Text Preparation ──┐
                   ├── Information Extraction
                   │
Classification ────┤
                   ├── RAG Pipeline ─── Agent Orchestration
Context Engineering┤                           │
                   │                           │
Memory Patterns ───┴─────────────── Evaluation │
                                               │
Prompt Engineering ────────────────────────────┘
```

RAG and Agent Orchestration are the core. Everything else supports them.

## What You'll Need

This phase uses more libraries:

```bash
# NLP
pip install spacy nltk keybert rapidfuzz

# ML/Embeddings
pip install sentence-transformers setfit

# Evaluation
pip install scikit-learn rouge-score

# Setup
python -m spacy download en_core_web_sm
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('wordnet')"
```

## The Job Market Analyzer

Throughout these modules, we keep building toward a system that can:
- "Find me ML engineer jobs in Europe that mention PyTorch"
- "How does the first one compare to the third?"
- "What's the typical salary range for these roles?"
- "Remember, I prefer remote positions"

Each module adds a capability. By the end, you have something that actually works.
