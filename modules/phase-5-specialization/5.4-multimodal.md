# Multimodal

Text isn't everything. Images contain information too. Multimodal AI works with both.

## Why This Matters

The moment I realized I could ask GPT-4V "what's in this receipt?" and get structured JSON back, document processing stopped feeling like a chore. No more OCR pipelines with regex nightmares. Just show the model the image and ask for what you need.

But multimodal goes beyond document extraction:
- **Image search**: Find images using natural language queries
- **Visual Q&A**: Answer questions about what's in an image
- **Document processing**: Extract invoices, forms, receipts
- **Content analysis**: Describe, classify, or moderate images

For the job market analyzer, multimodal could:
- Process job posts that include images (company logos, team photos)
- Extract data from PDF job descriptions that are image-based
- Search across job posts using visual similarity


## Scripts in This Module

| Script | What it shows |
|--------|---------------|
| <a href="../../scripts/phase-5-specialization/5.4-multimodal/01_vision_basics.py">01_vision_basics.py</a> | Analyze images with GPT-4V |
| <a href="../../scripts/phase-5-specialization/5.4-multimodal/02_clip_basics.py">02_clip_basics.py</a> | Text-image similarity with CLIP |
| <a href="../../scripts/phase-5-specialization/5.4-multimodal/03_image_text_search.py">03_image_text_search.py</a> | Build CLIP-based search |
| <a href="../../scripts/phase-5-specialization/5.4-multimodal/04_document_vision.py">04_document_vision.py</a> | Extract structured data from documents |


## The Key Ideas

### Vision-Language Models

GPT-4V, Claude, and Gemini can see images. You send them an image (as a URL or base64) alongside text, and they respond based on both:

```python
response = client.chat.completions.create(
    model="gpt-4-vision-preview",
    messages=[{
        "role": "user",
        "content": [
            {"type": "text", "text": "What's in this image?"},
            {"type": "image_url", "image_url": {"url": "https://..."}}
        ]
    }]
)
```

These models can:
- Describe image contents
- Answer questions about images
- Extract text (OCR)
- Analyze charts and diagrams
- Compare multiple images

The catch: they're expensive ($0.01-0.03 per image) and slow (several seconds).

### CLIP: Text-Image Embeddings

CLIP creates embeddings where text and images live in the same vector space. "A photo of a cat" is close to actual cat photos.

This enables:
- **Text → Image search**: Search photos using descriptions
- **Image → Image search**: Find similar images
- **Zero-shot classification**: Classify images by describing categories

CLIP is fast and free (after initial setup). You compute embeddings once and search many times.

### Document Vision

Structured extraction from document images:

```python
prompt = """Extract from this invoice:
{
  "invoice_number": "",
  "date": "",
  "total": 0,
  "items": []
}"""

data = analyze_image("invoice.jpg", prompt)
```

This replaces complex OCR + parsing pipelines with a single vision API call. The model understands document structure, not just text.

Use cases:
- Receipt processing
- Form extraction
- ID card reading
- Contract analysis

### Trade-offs

| Approach | Speed | Cost | Quality | Use Case |
|----------|-------|------|---------|----------|
| Vision API (GPT-4V) | Slow | High | Best | Analysis, extraction |
| CLIP | Fast | Free* | Good | Search, classification |
| Traditional OCR | Medium | Free* | Variable | Simple text extraction |

*After compute costs

## Things to Think About

- **When do I use vision APIs vs CLIP?** Vision APIs for understanding and extraction. CLIP for search and classification. They solve different problems.

- **How do I handle costs at scale?** Optimize image sizes (resize before encoding), use "low" detail mode when possible, cache results aggressively.

- **What about accuracy?** Vision models can hallucinate. They might miss details or make things up. Always validate critical extractions with human review.

- **Can I run vision locally?** Yes - models like LLaVA work locally. Quality is lower than GPT-4V but it's free and private.

## Related

- [Document Processing](../phase-3-advanced-patterns/3.9-document-processing.md) - PDF handling
- [Custom Embeddings](./5.2-custom-embeddings.md) - Combine with CLIP embeddings
- [Advanced RAG](../phase-3-advanced-patterns/3.4-advanced-rag.md) - Multimodal RAG
- [Cloud Deployment](../phase-4-production/4.7-cloud-deployment.md) - Deploy vision systems

## Book References

- hands_on_LLM.II.9 - Multimodal LLMs
- AI_eng.6 - Document processing
- AI_eng.9 - Vision applications
