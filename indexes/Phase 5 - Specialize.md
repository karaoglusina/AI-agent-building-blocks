

- [ ] **Module 5.1: Fine-tuning LLMs** `transformers`, `peft`, `bitsandbytes`
    
    - [ ] `01_when_to_finetune.py` - Decision framework: fine-tune vs RAG vs prompt engineering
    - [ ] `02_data_preparation.py` - Format data, data quality, data synthesis basics
    - [ ] `03_lora_basics.py` - Parameter-efficient fine-tuning with LoRA/QLoRA
    - [ ] `04_quantization.py` - Run models locally with reduced precision
    - [ ] `05_sft_rlhf_dpo_overview.py` - Understand alignment techniques (concepts)
    - [ ] `06_evaluation.py` - Measure improvement from fine-tuning
- [ ] **Module 5.2: Custom Embeddings** `sentence-transformers`
    
    - [ ] `01_sentence_transformers.py` - Use sentence-transformers models locally
    - [ ] `02_domain_adaptation_tsdae.py` - Unsupervised domain adaptation for embeddings with TSDAE
    - [ ] `03_embedding_evaluation.py` - Measure embedding quality
    - [ ] `04_bias_in_embeddings.py` - Awareness of embedding biases
- [ ] **Module 5.3: Advanced NLP** `spacy`, `coreferee` or `neuralcoref`
    
    - [ ] `01_dependency_parsing.py` - Extract grammatical structure with spaCy
    - [ ] `02_relation_extraction.py` - Extract relationships between entities
    - [ ] `03_coreference.py` - Resolve pronouns to entities
- [ ] **Module 5.4: Multimodal** `openai`, `transformers`, `open_clip`
    
    - [ ] `01_vision_basics.py` - Analyze images with GPT-4V, multimodal prompting
    - [ ] `02_clip_basics.py` - Text-image similarity with CLIP
    - [ ] `03_image_text_search.py` - CLIP-based multimodal search
    - [ ] `04_document_vision.py` - Extract info from document images