

- [ ] **Module 2.1: Text Preparation, cleaning, chunking. `nltk`, `spacy`, `tiktoken`, `re`** 
	- [ ] `01_text_cleaning.py` - Remove HTML, normalize whitespace, Unicode/encoding
	- [ ] `02_tokenization_nltk.py` - Word and sentence tokenization
	- [ ] `03_lemmatization_stemming.py` - Reduce words to base forms using spaCy lemma, NLTK Porter/Snowball
	- [ ] `04_stopwords.py` - Remove common words, custom stopword lists
	- [ ] `05_sentence_segmentation.py` - Split text into sentences using spaCy, NLTK
	- [ ] `06_chunking_fixed.py` - Split text by character/token count with overlap
	- [ ] `07_chunking_semantic.py` - Split by paragraphs, sections, or meaning boundaries
	- [ ] `08_chunking_recursive.py` - LangChain-style recursive split, implement from scratch
	- [ ] `09_metadata_extraction.py` - Extract title, dates, structure from documents
- [ ] **Module 2.2: Extract structure from unstructured text `spacy`, `nltk`, `keybert`, `scikit-learn`, `rapidfuzz`, `re`**
    - [ ] `01_ner_spacy.py` - Extract named entities (ORG, PERSON, LOCATION, etc.) with spaCy
    - [ ] `02_ner_custom.py` - Add custom entity rules and gazetteers to spaCy
    - [ ] `03_pos_tagging.py` - Tag words with grammatical categories (NOUN, VERB, ADJ, etc.)
    - [ ] `04_grammar_noun_chunks.py` - Extract noun phrases and grammatical structure
    - [ ] `05_keywords_tfidf.py` - Extract keywords using TF-IDF scores
    - [ ] `06_keywords_keybert.py` - Embedding-based keyword extraction
    - [ ] `07_lexicon_approaches.py` - Use word lists, dictionaries for extraction (sentiment lexicons, domain terms)
    - [ ] `08_extraction_llm.py` - Use structured output for flexible extraction with LLM
    - [ ] `09_regex_patterns.py` - Extract emails, phones, URLs, custom patterns
    - [ ] `10_fuzzy_matching.py` - Edit distance, fuzzy deduplication with rapidfuzz
- [ ] **Module 2.3: Clasify content and route to appropriate handlers `openai`, `sentence-transformers`, `setfit`**
    - [ ] `01_zero_shot.py` - Classify without training data using LLM
    - [ ] `02_few_shot.py` - Provide examples for better accuracy in classification
    - [ ] `03_setfit_fewshot.py` - Train classifier with minimal examples using SetFit
    - [ ] `04_intent_detection.py` - Detect user intent from queries
    - [ ] `05_query_routing.py` - Route queries to different handlers based on type
    - [ ] `06_sentiment.py` - Classify positive/negative/neutral sentiment
- [ ] **Module 2.4: RAG Pipeline with chromaDB and openAI**
    - [ ] `01_basic_rag.py` - End-to-end retrieve → augment → generate
    - [ ] `02_context_assembly.py` - Format retrieved docs for the prompt
    - [ ] `03_source_citation.py` - Include references in generated answers
    - [ ] `04_no_results_handling.py` - Graceful fallback when retrieval fails
    - [ ] `05_rag_with_filters.py` - Combine semantic search with structured metadata filters
    - [ ] `06_rag_evaluation.py` - Measure retrieval quality (MRR, recall@k, precision@k)
- [ ] **Module 2.5: Agent Orchestration, `asyncio`**
    - [ ] `01_tool_calling_loop.py` - Basic agent loop: think → act → observe
    - [ ] `02_sequential_chain.py` - Chain multiple LLM calls in sequence
    - [ ] `03_conditional_routing.py` - Route to different paths based on LLM decision
    - [ ] `04_parallel_execution.py` - Run multiple LLM calls concurrently
    - [ ] `05_tool_with_context.py` - Pass context between tool calls
    - [ ] `06_error_recovery.py` - Handle tool failures, common agent failure patterns
    - [ ] `07_agent_with_rag.py` - Agent that can search knowledge base
    - [ ] `08_multi_tool_agent.py` - Agent with multiple capabilities and tool selection strategies
- [ ] **Module 2.6: Context Engineering `tiktoken`**
    - [ ] `01_context_window_basics.py` - Token limits, counting, efficiency considerations
    - [ ] `02_prompt_assembly.py` - Build prompts from components dynamically
    - [ ] `03_context_prioritization.py` - Decide what to include when space is limited
    - [ ] `04_context_compression.py` - Summarize context to fit more information
    - [ ] `05_dynamic_system_prompts.py` - Adjust system prompt based on task
- [ ] **Module 2.7: Memory Patterns**
    - [ ] `01_conversation_buffer.py` - Store full conversation history
    - [ ] `02_sliding_window.py` - Keep only recent N messages, window sizing strategies
    - [ ] `03_summary_memory.py` - Summarize old conversations
    - [ ] `04_entity_memory.py` - Track entities mentioned in conversation
    - [ ] `05_long_term_storage.py` - Persist important facts to database
    - [ ] `06_memory_retrieval.py` - Retrieve relevant memories for current context
- [ ] **Module 2.8: Prompt Engineering `instructor` **
    - [ ] `01_system_prompt_design.py` - Structure effective system prompts
    - [ ] `02_chain_of_thought.py` - Make the model reason step-by-step
    - [ ] `03_few_shot_examples.py` - Provide examples for consistent output
    - [ ] `04_output_formatting.py` - Control output structure without schemas
    - [ ] `05_instructor_basics.py` - Model-agnostic structured output with Instructor library
    - [ ] `06_constrained_generation.py` - Force output to follow patterns/grammar
    - [ ] `07_defensive_prompting.py` - Protect against misuse and prompt attacks
    - [ ] `08_self_consistency.py` - Sample multiple outputs and aggregate
- [ ] **Module 2.9: Evaluation Basics**  `scikit-learn`, `rouge-score`
    - [ ] `01_classification_metrics.py` - Precision, recall, F1, confusion matrix
    - [ ] `02_retrieval_metrics.py` - MRR, MAP, NDCG, recall@k, precision@k
    - [ ] `03_generation_metrics.py` - BLEU, ROUGE for text generation evaluation
    - [ ] `04_llm_as_judge.py` - Use LLM to evaluate outputs
    - [ ] `05_simple_eval_loop.py` - Run evaluation on test cases
    - [ ] `06_comparison_eval.py` - Compare two approaches with A/B testing
    - [ ] `07_embedding_evaluation.py` - Evaluate embedding quality for your domain